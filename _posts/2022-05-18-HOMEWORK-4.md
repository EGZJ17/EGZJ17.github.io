---
layout: post
title: "Classifying Fakes News from Real News"
---

*In this blog post, I will build a fake news detector using TensorFlow word embedding and visualize the embedding using Plotly.*

# §1. Setup
First, we need to import all the necessary packages:
```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup
from tensorflow import keras

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# for stopwords
from nltk.corpus import stopwords

# for embedding viz
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
import matplotlib.pyplot as plt
```

## Acquiring Training Data 
Next, let’s go ahead and read in our training data and take a look at the dataset we have been given.
```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url)
df.head()
```
We see that each row of the data corresponds to an article. The title column gives the title of the article, while the text column gives the full article text. The final column, called fake, is 0 if the article is true and 1 if the article contains fake news, as determined by the authors of the paper above.


# §2. Make the Dataset
Our next step is to remove the stopwords from the `title` and `text` columns. A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.” 
To do this we first run the following code: 
``` python 
import nltk
nltk.download('stopwords')
stop = stopwords.words('english')
```
```
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.
```

Now let's construct and return a `tf.data.Dataset` with two inputs and one output. The input will be of the form (title, text), and the output will consist only of the fake column. We will also batch our data to help increase the speed of training.
``` python
def make_dataset(df):
  '''
  removes stopwords from df and then converts
  the dataframe to a tf.data.dataset.
  '''
  # remove stopwords
  df['title_without_stopwords'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df['text_without_stopwords'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

  # specify inputs and outputs of tf.data.dataset
  my_data_set = tf.data.Dataset.from_tensor_slices(
      ( # dictionary for input data/features
          {
              "title" : df[["title_without_stopwords"]], 
              "text" : df[["text_without_stopwords"]]
          }, 
           # dictionary for output data/labels
          {
              "fake" : df[["fake"]]
          }
      )
  )
  my_data_set = my_data_set.batch(100)
  return my_data_set
  ```

## Split the Dataset
Now, let’s call our function and construct our dataset. Then we need to split it into train, validation, and test. This is done below, and 20% of our training data is set aside as validation data.
``` python
data = make_dataset(df) 

# 70% train, 20% validation, 10% test
train_size = int(0.7*len(data)) 
val_size = int(0.2*len(data))

# get validation and training and test data
train = data.take(train_size) # data[:train_size]
val = data.skip(train_size).take(val_size) # data[train_size : train_size + val_size]
test = data.skip(train_size+val_size) #  data[train_size + val_size:]
```

## Base Rate 
