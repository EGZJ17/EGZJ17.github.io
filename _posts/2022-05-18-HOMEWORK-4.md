---
layout: post
title: "Classifying Fakes News from Real News"
---

*In this blog post, I will build a fake news detector using TensorFlow word embedding and visualize the embedding using Plotly.*

# §1. Setup
First, we need to import all the necessary packages:
```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup
from tensorflow import keras

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# for stopwords
from nltk.corpus import stopwords

# for embedding viz
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
import matplotlib.pyplot as plt
```

## Acquiring Training Data 
Next, let’s go ahead and read in our training data and take a look at the dataset we have been given.
```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url)
df.head()
```
We see that each row of the data corresponds to an article. The title column gives the title of the article, while the text column gives the full article text. The final column, called fake, is 0 if the article is true and 1 if the article contains fake news, as determined by the authors of the paper above.


# §2. Make the Dataset
Our next step is to remove the stopwords from the `title` and `text` columns. A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.” 
To do this we first run the following code: 
``` python 
import nltk
nltk.download('stopwords')
stop = stopwords.words('english')
```
```
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.
```

Now let's construct and return a `tf.data.Dataset` with two inputs and one output. The input will be of the form (title, text), and the output will consist only of the fake column. We will also batch our data to help increase the speed of training.
``` python
def make_dataset(df):
  '''
  removes stopwords from df and then converts
  the dataframe to a tf.data.dataset.
  '''
  # remove stopwords
  df['title_without_stopwords'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df['text_without_stopwords'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

  # specify inputs and outputs of tf.data.dataset
  my_data_set = tf.data.Dataset.from_tensor_slices(
      ( # dictionary for input data/features
          {
              "title" : df[["title_without_stopwords"]], 
              "text" : df[["text_without_stopwords"]]
          }, 
           # dictionary for output data/labels
          {
              "fake" : df[["fake"]]
          }
      )
  )
  my_data_set = my_data_set.batch(100)
  return my_data_set
  ```

## Split the Dataset
Now, let’s call our function and construct our dataset. Then we need to split it into train, validation, and test. This is done below, and 20% of our training data is set aside as validation data.
``` python
data = make_dataset(df) 

# 70% train, 20% validation, 10% test
train_size = int(0.7*len(data)) 
val_size = int(0.2*len(data))

# get validation and training and test data
train = data.take(train_size) # data[:train_size]
val = data.skip(train_size).take(val_size) # data[train_size : train_size + val_size]
test = data.skip(train_size+val_size) #  data[train_size + val_size:]
```

## Base Rate 
The base rate refers to the accuracy of a model that always makes the same guess (for example, such a model might always say “fake news!”). Now we need to determine the base rate for this data set by examining the labels on the training set.
``` python
#iterate through the labels on the training data 
labels_iterator= train.unbatch().map(lambda text, fake: fake).as_numpy_iterator()

true = 0
fake = 0

for labels in labels_iterator:
  #if label = 0, add to true
  if labels['fake'] == 0:
      true += 1
  #if label = 1, add to fake
  else:
      fake += 1

print(str("Articles labeled as true:"), true)
print(str("Articles labeled as fake:"), fake)

#how often will the model identify an article as true
base_rate = fake / (true + fake)
base_rate = str(round(base_rate*100, 2))
print(str("The base model will predict 'fake'"), base_rate, str("% of the time."))
```
```
Articles labeled as true: 7483
Articles labeled as fake: 8217
The base model will predict 'fake' 52.34 % of the time.
```
There are 7,483 articles labeled as true. There are 8,217 articles labeled as fake.  Thus, the base model will predict an article is 'fake' 52.34 % of the time.

# §3. Create Models
## Model 1
In the first model, we will use **only the article title** as an input.

We will also add the following code to vectorize our layers and format them so that we can adapt them for our models.
``` python
#preparing a text vectorization layer for tf model
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)   # convert into lowercase
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    # remove punctuation and some other elements
    return no_punctuation 

title_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

title_vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
```

Our first model uses only the title of the articles to predict whether an article is fake or not. We will pass our title_input through the following layers and then predict the output using the following code.
``` python
title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

title_features = title_vectorize_layer(title_input) # apply the vectorization layer to the titles_input
title_features = layers.Embedding(size_vocabulary, output_dim = 3, name="embedding1")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
output = layers.Dense(2, name = "fake")(title_features)

model1 = keras.Model(
    inputs = title_input,
    outputs = output
)
```
Now we have to fit our model to our training set.
``` python
model1.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

history = model1.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = 1)
```
```
```

Let's look at the flow of our model
``` python
from tensorflow.keras import utils
utils.plot_model(model1)
```
```
```

Finally, let's plot its performance on the validation set.
``` python
def history_plot():
  plt.plot(history.history["accuracy"], label = "training")
  plt.plot(history.history["val_accuracy"], label = "validation")
  plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
  plt.legend()

history_plot()
```
Looks like we got our model ton be fairly accurate! Now let’s perform the same process but this time we will only use the text of the articles in our model.

## Model 2
In the second model, we will use **only the article text** as an input.

Now, we will add the following code to vectorize our layers and format them so that we can adapt them for our models.
``` python
text_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

text_vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
```

Our second model uses only the text of the articles to predict whether an article is fake or not. 
``` python
text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)

text_features = text_vectorize_layer(text_input) # apply the vectorization layer to the titles_input
text_features = layers.Embedding(size_vocabulary, output_dim = 10, name="embedding2")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
output = layers.Dense(2, name = "fake")(text_features)

model2 = keras.Model(
    inputs = text_input,
    outputs = output
)
```
Now we have to fit our model to our training set.
``` python
model2.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

history = model2.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = 1)
```
```
```
Let's look at the flow of our model
``` python
utils.plot_model(model2)
```
```
```

Finally, let's plot its performance on the validation set.
``` python
history_plot()
```
```
```
Once again, looks pretty good! For our last model, we will take into account both the titles and the text when we construct our model. We predict that this will be our most accurate model yet.

## Model 3
In the third model, we will use **both the article title and the article text** as input.
Now we can combine the two models using layers.concatenate.
``` python
title_features = title_features
text_features = text_features

# combine text and title
main = layers.concatenate([title_features, text_features], axis = 1)

# output
main = layers.Dense(32, activation='relu')(main)
output = layers.Dense(2, name = "fake")(main)

model3 = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)
```
Now we have to fit our model to our training set.
``` python
model3.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

history = model3.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = 1)
```
```
```
Let's look at the flow of our model
``` python
utils.plot_model(model3)
```
```
```

Finally, let's plot its performance on the validation set.
``` python
history_plot()
```
```
```

# §4. Model Evaluation
Now let's test our models performance on unseen test data. 
``` python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true" #import test data
test_pd = pd.read_csv(train_url, index_col = 0)
test_data = make_dataset(test_pd)

loss1, accuracy1 = model1.evaluate(test_data)
print('Test accuracy for Model 1 :', accuracy1)

loss2, accuracy2 = model2.evaluate(test_data)
print('Test accuracy for Model 2 :', accuracy2)

loss3, accuracy3 = model3.evaluate(test_data)
print('Test accuracy for Model 3 :', accuracy3)
```
```
225/225 [==============================] - 1s 3ms/step - loss: 0.0479 - accuracy: 0.9881
Test accuracy for Model 1 : 0.9880618453025818
225/225 [==============================] - 2s 8ms/step - loss: 0.1199 - accuracy: 0.9760
Test accuracy for Model 2 : 0.9759899973869324
225/225 [==============================] - 2s 9ms/step - loss: 0.0126 - accuracy: 0.9968
Test accuracy for Model 3 : 0.9968372583389282
```

We see that the test accuracy for Model3 is 99.68% which is expected since it is our best model.


# §5. Embedding Visualization
Our final step is to visualize the embeddings using PCA to reduce the features to 2-dimensional weights and take a closer look at some of the words that were most associated with fake news articles.
``` python
weights = model3.get_layer('embedding1').get_weights()[0] # get the weights from the embedding layer
vocab = title_vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later

from sklearn.decomposition import PCA
pca = PCA(n_components=2)     # Convert our data into 2 dimensions
weights = pca.fit_transform(weights)

# visualzing the text embedding
embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})

import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = [2]*len(embedding_df),
                # size_max = 2,
                 hover_name = "word")

fig.show()
```
```
```
The words that are far from the orgin suggests strong indication towards fake or true news. On the negative x0 side we have “trumps” and “thats” while the “gop” and “21st” is on the postive x0 side. Near the origin, we have “leaders”. None of the words show strong indiction towards fake or true news. Therefore, fake news might be very hard to be identified by humans but easier detected by machine learning.